
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{ulem} % ulem is needed to support strikethroughs (\sout)
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Analysis}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1536}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
           \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
           \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
           \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
\end{Verbatim}

    \subsubsection{Loading the Datasets}\label{loading-the-datasets}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2556}]:} \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{from\PYZus{}csv}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{../data/train\PYZus{}risk.csv}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{from\PYZus{}csv}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{../data/test\PYZus{}risk.csv}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
\end{Verbatim}

    \subsection{Analysing the Data}\label{analysing-the-data}

Looking at the difference between the number of positive and negative
samples in the dataset shows that there are more negative examples than
positive examples. Only 28\% of all samples are of the positive class.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}428}]:} \PY{k}{def} \PY{n+nf}{class\PYZus{}balance\PYZus{}summary}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Summarise the imbalance in the dataset\PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{n}{total\PYZus{}size} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{size}
              \PY{n}{negative\PYZus{}class} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}
              \PY{n}{positive\PYZus{}class} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{y} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}
              \PY{n}{ratio} \PY{o}{=} \PY{n}{positive\PYZus{}class} \PY{o}{/} \PY{n+nb}{float}\PY{p}{(}\PY{n}{positive\PYZus{}class} \PY{o}{+} \PY{n}{negative\PYZus{}class}\PY{p}{)}
          
              \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Total number of samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{total\PYZus{}size}
              \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Number of positive samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{positive\PYZus{}class}
              \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Number of negative samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{negative\PYZus{}class}
              \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Ratio of positive to total number of samples: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{ratio}
          
          
          \PY{n}{class\PYZus{}balance\PYZus{}summary}\PY{p}{(}\PY{n}{y}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Total number of samples: 300
Number of positive samples: 84
Number of negative samples: 216
Ratio of positive to total number of samples: 0.28
    \end{Verbatim}

    Some initial observations about the data before it is preprocessed: -
PRE32 is all zeros. This can be removed - PRE14 looks catagorical.
Should be split into multiple binary variables - DGN looks catagorical.
As above. - PRE5 looks to have some outliers. See box plot below.
Potentially remove or split into two extra variable?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2391}]:} \PY{n}{X}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2391}]:}    DGN  PRE4  PRE5  PRE6  PRE7  PRE8  PRE9  PRE10  PRE11  PRE14  PRE17  PRE19  \textbackslash{}
           0    3  2.08  1.52     1     0     0     0      1      0      4      0      0   
           1    3  4.36  3.76     0     0     0     0      0      0      1      0      0   
           2    3  3.52  2.28     0     0     0     0      0      0      3      0      0   
           3    3  3.36  2.67     1     0     0     0      1      0      1      0      0   
           4    3  3.56  2.80     0     0     0     0      0      0      2      0      0   
           
              PRE25  PRE30  PRE32  AGE  
           0      0      1      0   49  
           1      0      1      0   72  
           2      0      1      0   51  
           3      0      1      0   72  
           4      0      0      0   69  
\end{Verbatim}
        
    Box plot below shows the outliers in PRE5. It is worth noting that all
of these outliers are of the negative class. This variable is the volume
that can be exhaled in one second given full inhilation. It is likely
that these values are therefore errors in reporting as it is unlikely
that humans can exhale such a large volume so quickly.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3501}]:} \PY{c}{\PYZsh{} X.PRE5.plot(kind=\PYZsq{}box\PYZsq{})}
           \PY{n}{X}\PY{o}{.}\PY{n}{PRE5}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{box}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{k}{print} \PY{n}{y}\PY{p}{[}\PY{n}{X}\PY{o}{.}\PY{n}{PRE5} \PY{o}{\PYZgt{}} \PY{l+m+mi}{30} \PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
11     0
25     0
101    0
108    0
122    0
212    0
240    0
Name: Risk1Yr, dtype: int64
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_8_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Preprocessing}\label{preprocessing}

Create a new matrix of preprocessed features. This will encode
catagorical data as one hot vectors, remove outliers, and normalise the
data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4081}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{preprocessing}
           
           \PY{k}{def} \PY{n+nf}{encode\PYZus{}onehot}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{column\PYZus{}name}\PY{p}{,} \PY{n}{digitize}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}\PY{p}{:}
               \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Encode a catagorical column from a data frame into a data frame of one hot features\PYZdq{}\PYZdq{}\PYZdq{}}
               \PY{n}{data} \PY{o}{=} \PY{n}{x\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{n}{column\PYZus{}name}\PY{p}{]}\PY{p}{]}
           
               \PY{k}{if} \PY{n}{digitize}\PY{p}{:}
                   \PY{n}{data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{digitize}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{data}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
               
               \PY{n}{enc} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{OneHotEncoder}\PY{p}{(}\PY{p}{)}
               \PY{n}{features} \PY{o}{=} \PY{n}{enc}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
               \PY{n}{names} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s}{\PYZus{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{column\PYZus{}name}\PY{p}{,} \PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{enc}\PY{o}{.}\PY{n}{active\PYZus{}features\PYZus{}}\PY{p}{]}
               \PY{n}{features} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{names}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{)}
               \PY{k}{return} \PY{n}{features}
           
           
           \PY{k}{def} \PY{n+nf}{preprocess}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}\PY{p}{:}
               \PY{c}{\PYZsh{} drop zero var PRE32}
               \PY{n}{Xp} \PY{o}{=} \PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{PRE32}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
               
               \PY{c}{\PYZsh{} remove outliers}
               \PY{k}{if} \PY{n}{y\PYZus{}data} \PY{o+ow}{is} \PY{o+ow}{not} \PY{n+nb+bp}{None}\PY{p}{:} 
                   \PY{n}{mask} \PY{o}{=} \PY{n}{Xp}\PY{o}{.}\PY{n}{PRE5} \PY{o}{\PYZlt{}} \PY{l+m+mi}{30}
                   \PY{n}{Xp} \PY{o}{=} \PY{n}{Xp}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
                   \PY{n}{Yp} \PY{o}{=} \PY{n}{y\PYZus{}data}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                   \PY{n}{Yp} \PY{o}{=} \PY{n}{Yp}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
               \PY{k}{else}\PY{p}{:}
                   \PY{n}{Yp} \PY{o}{=} \PY{n+nb+bp}{None}
               
               \PY{c}{\PYZsh{} encode catagorical data as one hot vectors}
               \PY{n}{one\PYZus{}hot\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{DGN}\PY{l+s}{\PYZdq{}}\PY{p}{]}
               \PY{n}{encoded} \PY{o}{=} \PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{name}\PY{p}{:} \PY{n}{encode\PYZus{}onehot}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{name}\PY{p}{)}\PY{p}{,} \PY{n}{one\PYZus{}hot\PYZus{}names}\PY{p}{)}
               \PY{c}{\PYZsh{}combine into a single data frame}
               \PY{n}{new\PYZus{}features} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{encoded}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           
               \PY{c}{\PYZsh{} drop the catagorical variables that have been encoded}
               \PY{n}{Xp}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{DGN}\PY{l+s}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
               \PY{c}{\PYZsh{} add new features}
               \PY{n}{Xp} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xp}\PY{p}{,} \PY{n}{new\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           
               \PY{k}{return} \PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}
           
           \PY{n}{Xp}\PY{p}{,} \PY{n}{Yp} \PY{o}{=} \PY{n}{preprocess}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
           \PY{n}{Xp}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4081}]:}    PRE4  PRE5  PRE6  PRE7  PRE8  PRE9  PRE10  PRE11  PRE14  PRE17  {\ldots}    \textbackslash{}
           0  2.08  1.52     1     0     0     0      1      0      4      0  {\ldots}     
           1  4.36  3.76     0     0     0     0      0      0      1      0  {\ldots}     
           2  3.52  2.28     0     0     0     0      0      0      3      0  {\ldots}     
           3  3.36  2.67     1     0     0     0      1      0      1      0  {\ldots}     
           4  3.56  2.80     0     0     0     0      0      0      2      0  {\ldots}     
           
              PRE25  PRE30  AGE  DGN\_1  DGN\_2  DGN\_3  DGN\_4  DGN\_5  DGN\_6  DGN\_8  
           0      0      1   49    0.0    0.0    1.0    0.0    0.0    0.0    0.0  
           1      0      1   72    0.0    0.0    1.0    0.0    0.0    0.0    0.0  
           2      0      1   51    0.0    0.0    1.0    0.0    0.0    0.0    0.0  
           3      0      1   72    0.0    0.0    1.0    0.0    0.0    0.0    0.0  
           4      0      0   69    0.0    0.0    1.0    0.0    0.0    0.0    0.0  
           
           [5 rows x 21 columns]
\end{Verbatim}
        
    Measure the effectiveness of each feature using the variable importance
measure from a Random Forest

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4043}]:} \PY{k}{def} \PY{n+nf}{measure\PYZus{}importance}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{)}\PY{p}{:}
               \PY{n}{rf\PYZus{}selector} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{gini}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{)}
               \PY{n}{rf\PYZus{}selector}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{)}
               \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{rf\PYZus{}selector}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
               \PY{n}{feature\PYZus{}importance}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{bar}\PY{l+s}{\PYZsq{}}\PY{p}{)}
               \PY{k}{return} \PY{n}{feature\PYZus{}importance}
           
           \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{measure\PYZus{}importance}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           \PY{n}{Xp}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{[}\PY{n}{feature\PYZus{}importance} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3967}]:} \PY{n}{feature\PYZus{}importance}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{barh}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Gini Impurity}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/feature\PYZus{}importance.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The numerical features appear to be the most important ones. Plot a
scatter plot matrix to see how the how the correlate with each other

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3757}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{tools}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{Xp}\PY{p}{[}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{PRE4}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE5}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{AGE}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3757}]:} array([[<matplotlib.axes.\_subplots.AxesSubplot object at 0x1756c3dd0>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x17a46f850>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x1a4388f50>],
                  [<matplotlib.axes.\_subplots.AxesSubplot object at 0x198cbfe90>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x17829be10>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x19eb01ad0>],
                  [<matplotlib.axes.\_subplots.AxesSubplot object at 0x17c32da50>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x1767ab8d0>,
                   <matplotlib.axes.\_subplots.AxesSubplot object at 0x188c8bad0>]], dtype=object)
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Tuning Model Parameters}\label{tuning-model-parameters}

Given the current status of the data tune the model parameters to it
before we evalute the overall performance. Note that all of the tuning
presented here is orientated towards obtaining the highest AUC score.
Other metrics might be more desirable given the problem domain, but AUC
is the measurement used for assignment points.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3532}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{cross\PYZus{}validation}
           \PY{n}{skf} \PY{o}{=} \PY{n}{cross\PYZus{}validation}\PY{o}{.}\PY{n}{StratifiedKFold}\PY{p}{(}\PY{n}{Yp}\PY{p}{,} \PY{n}{n\PYZus{}folds}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \subsubsection{Random Forest Tuning}\label{random-forest-tuning}

Run a grid search over a range of parameters for a Random Forest. The
dataset is small enough that we can do them all at once.
\texttt{n\_estimators} is neglected because this should always improve
as it is increased so we should attempt to make it as large as possible
subject to lack of improvement

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3835}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                        \PY{p}{\PYZcb{}}
           
           \PY{n}{rf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
           \PY{n}{rf\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{rf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{rf\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3835}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=RandomForestClassifier(bootstrap=True, class\_weight='balanced',
                       criterion='gini', max\_depth=None, max\_features='auto',
                       max\_leaf\_nodes=None, min\_samples\_leaf=1, min\_samples\_split=2,
                       min\_weight\_fraction\_leaf=0.0, n\_estimators=50, n\_jobs=1,
                       oob\_score=False, random\_state=50, verbose=0, warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=-1,
                  param\_grid=\{'max\_features': [2, 5, 8, 11, 14, 17], 'min\_samples\_split': [1, 2, 3, 4], 'max\_depth': [1, 4, 7, 10, 13, 16, 19], 'min\_samples\_leaf': [1, 2, 3, 4]\},
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3836}]:} \PY{k}{print} \PY{n}{rf\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'max\_features': 5, 'min\_samples\_split': 1, 'max\_depth': 16, 'min\_samples\_leaf': 1\}
    \end{Verbatim}

    Now take a look at the number of estimators and see where performance
begins to level off.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3838}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{\PYZcb{}}
           \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{16}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}
           
           \PY{n}{rf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           \PY{n}{rf\PYZus{}clf2} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{rf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3838}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=RandomForestClassifier(bootstrap=True, class\_weight='balanced',
                       criterion='gini', max\_depth=16, max\_features=1,
                       max\_leaf\_nodes=None, min\_samples\_leaf=1, min\_samples\_split=1,
                       min\_weight\_fraction\_leaf=0.0, n\_estimators=50, n\_jobs=1,
                       oob\_score=False, random\_state=50, verbose=0, warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=-1,
                  param\_grid=\{'n\_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450]\},
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    The best parameters for \texttt{n\_estimators} levels off after around
300 estimators

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3840}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{)}
           \PY{k}{print} \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{k}{print} \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'n\_estimators': 100\}
0.89217869367
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3725}]:} \PY{n}{rf\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3725}]:} \{'bootstrap': True,
            'class\_weight': 'balanced',
            'criterion': 'gini',
            'max\_depth': 16,
            'max\_features': 1,
            'max\_leaf\_nodes': None,
            'min\_samples\_leaf': 1,
            'min\_samples\_split': 1,
            'min\_weight\_fraction\_leaf': 0.0,
            'n\_estimators': 300,
            'n\_jobs': 1,
            'oob\_score': False,
            'random\_state': 50,
            'verbose': 0,
            'warm\_start': False\}
\end{Verbatim}
        
    \subsubsection{Gradient Boosting Tuning}\label{gradient-boosting-tuning}

Gradient boosting is difficult to tune effectively.
\href{http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/}{This
guide} suggests starting by fixing the learning rate and number of
estimators to a relatively low number in order to tune the other
hyperparameters. After they are optimised the learning rate is gradually
lowered and the number of estimators increased until we find convergance
on the optimum parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3587}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
              \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{\PYZcb{}}
           \PY{p}{]}
           
           \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{sqrt}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}\PY{p}{\PYZcb{}}
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           
           \PY{n}{gbc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{gbc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3587}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=GradientBoostingClassifier(init=None, learning\_rate=0.1, loss='deviance',
                         max\_depth=8, max\_features='sqrt', max\_leaf\_nodes=None,
                         min\_samples\_leaf=3, min\_samples\_split=1,
                         min\_weight\_fraction\_leaf=0.0, n\_estimators=100,
                         presort='auto', random\_state=50, subsample=0.8, verbose=0,
                         warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=1,
                  param\_grid=[\{'n\_estimators': [20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140]\}],
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \texttt{n\_estimators} plateaus at around 100, so we'll use this instead
of the optimum as less trees == quicker training and we'll need to
decrease the learning rate and increase the number of trees later in the
tuning anyway.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3588}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{)}
           \PY{k}{print} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'n\_estimators': 120\}
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now tune the \texttt{max\_depth} and the \texttt{min\_samples\_split}
parameters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3594}]:} \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.1}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{sqrt}\PY{l+s}{\PYZsq{}}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}
                          \PY{p}{\PYZcb{}}
           
           \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{\PYZcb{}}
           \PY{p}{]}
           
           
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{gbc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3594}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=GradientBoostingClassifier(init=None, learning\_rate=0.1, loss='deviance',
                         max\_depth=3, max\_features='sqrt', max\_leaf\_nodes=None,
                         min\_samples\_leaf=3, min\_samples\_split=2,
                         min\_weight\_fraction\_leaf=0.0, n\_estimators=100,
                         presort='auto', random\_state=50, subsample=0.8, verbose=0,
                         warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=1,
                  param\_grid=[\{'min\_samples\_split': [1, 4, 7, 10, 13, 16, 19], 'max\_depth': [5, 7, 9, 11, 13, 15]\}],
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3595}]:} \PY{k}{print} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'min\_samples\_split': 7, 'max\_depth': 9\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3595}]:} [mean: 0.88220, std: 0.04754, params: \{'min\_samples\_split': 1, 'max\_depth': 5\},
            mean: 0.88220, std: 0.04754, params: \{'min\_samples\_split': 4, 'max\_depth': 5\},
            mean: 0.87967, std: 0.04768, params: \{'min\_samples\_split': 7, 'max\_depth': 5\},
            mean: 0.89035, std: 0.04813, params: \{'min\_samples\_split': 10, 'max\_depth': 5\},
            mean: 0.87719, std: 0.05057, params: \{'min\_samples\_split': 13, 'max\_depth': 5\},
            mean: 0.87051, std: 0.05437, params: \{'min\_samples\_split': 16, 'max\_depth': 5\},
            mean: 0.86807, std: 0.05291, params: \{'min\_samples\_split': 19, 'max\_depth': 5\},
            mean: 0.89821, std: 0.04452, params: \{'min\_samples\_split': 1, 'max\_depth': 7\},
            mean: 0.89821, std: 0.04452, params: \{'min\_samples\_split': 4, 'max\_depth': 7\},
            mean: 0.89473, std: 0.04767, params: \{'min\_samples\_split': 7, 'max\_depth': 7\},
            mean: 0.89704, std: 0.04538, params: \{'min\_samples\_split': 10, 'max\_depth': 7\},
            mean: 0.89991, std: 0.03903, params: \{'min\_samples\_split': 13, 'max\_depth': 7\},
            mean: 0.88864, std: 0.05408, params: \{'min\_samples\_split': 16, 'max\_depth': 7\},
            mean: 0.87621, std: 0.05107, params: \{'min\_samples\_split': 19, 'max\_depth': 7\},
            mean: 0.89880, std: 0.04033, params: \{'min\_samples\_split': 1, 'max\_depth': 9\},
            mean: 0.89880, std: 0.04033, params: \{'min\_samples\_split': 4, 'max\_depth': 9\},
            mean: 0.90692, std: 0.03731, params: \{'min\_samples\_split': 7, 'max\_depth': 9\},
            mean: 0.89665, std: 0.04752, params: \{'min\_samples\_split': 10, 'max\_depth': 9\},
            mean: 0.88990, std: 0.05752, params: \{'min\_samples\_split': 13, 'max\_depth': 9\},
            mean: 0.88361, std: 0.05478, params: \{'min\_samples\_split': 16, 'max\_depth': 9\},
            mean: 0.88795, std: 0.05857, params: \{'min\_samples\_split': 19, 'max\_depth': 9\},
            mean: 0.90182, std: 0.04990, params: \{'min\_samples\_split': 1, 'max\_depth': 11\},
            mean: 0.90182, std: 0.04990, params: \{'min\_samples\_split': 4, 'max\_depth': 11\},
            mean: 0.90548, std: 0.04183, params: \{'min\_samples\_split': 7, 'max\_depth': 11\},
            mean: 0.90343, std: 0.04153, params: \{'min\_samples\_split': 10, 'max\_depth': 11\},
            mean: 0.90027, std: 0.03692, params: \{'min\_samples\_split': 13, 'max\_depth': 11\},
            mean: 0.90187, std: 0.04195, params: \{'min\_samples\_split': 16, 'max\_depth': 11\},
            mean: 0.88890, std: 0.04956, params: \{'min\_samples\_split': 19, 'max\_depth': 11\},
            mean: 0.90676, std: 0.03600, params: \{'min\_samples\_split': 1, 'max\_depth': 13\},
            mean: 0.90676, std: 0.03600, params: \{'min\_samples\_split': 4, 'max\_depth': 13\},
            mean: 0.89853, std: 0.04138, params: \{'min\_samples\_split': 7, 'max\_depth': 13\},
            mean: 0.89806, std: 0.04020, params: \{'min\_samples\_split': 10, 'max\_depth': 13\},
            mean: 0.89211, std: 0.02916, params: \{'min\_samples\_split': 13, 'max\_depth': 13\},
            mean: 0.89481, std: 0.03565, params: \{'min\_samples\_split': 16, 'max\_depth': 13\},
            mean: 0.89722, std: 0.04099, params: \{'min\_samples\_split': 19, 'max\_depth': 13\},
            mean: 0.90546, std: 0.04644, params: \{'min\_samples\_split': 1, 'max\_depth': 15\},
            mean: 0.90546, std: 0.04644, params: \{'min\_samples\_split': 4, 'max\_depth': 15\},
            mean: 0.90100, std: 0.04406, params: \{'min\_samples\_split': 7, 'max\_depth': 15\},
            mean: 0.89495, std: 0.04222, params: \{'min\_samples\_split': 10, 'max\_depth': 15\},
            mean: 0.89991, std: 0.02674, params: \{'min\_samples\_split': 13, 'max\_depth': 15\},
            mean: 0.89207, std: 0.03377, params: \{'min\_samples\_split': 16, 'max\_depth': 15\},
            mean: 0.89242, std: 0.04819, params: \{'min\_samples\_split': 19, 'max\_depth': 15\}]
\end{Verbatim}
        
    Now train \texttt{max\_features}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3600}]:} \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.1}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{sqrt}\PY{l+s}{\PYZsq{}}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{9}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{7}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}
                          \PY{p}{\PYZcb{}}
           
           \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{\PYZcb{}}
           \PY{p}{]}
           
           
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{gbc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3600}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=GradientBoostingClassifier(init=None, learning\_rate=0.1, loss='deviance',
                         max\_depth=9, max\_features='sqrt', max\_leaf\_nodes=None,
                         min\_samples\_leaf=3, min\_samples\_split=7,
                         min\_weight\_fraction\_leaf=0.0, n\_estimators=100,
                         presort='auto', random\_state=50, subsample=0.8, verbose=0,
                         warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=1,
                  param\_grid=[\{'max\_features': [5, 7, 9, 11, 13, 15, 17, 19]\}],
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3601}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{)}
           \PY{k}{print} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'max\_features': 11\}
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now train to tune the \texttt{subsample} rate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3603}]:} \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.1}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{sqrt}\PY{l+s}{\PYZsq{}}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{9}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{7}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{11}\PY{p}{,}
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}
                          \PY{p}{\PYZcb{}}
           
           \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.75}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.85}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{]}\PY{p}{\PYZcb{}}
           \PY{p}{]}
           
           
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{gbc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3603}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=GradientBoostingClassifier(init=None, learning\_rate=0.1, loss='deviance',
                         max\_depth=9, max\_features=11, max\_leaf\_nodes=None,
                         min\_samples\_leaf=3, min\_samples\_split=7,
                         min\_weight\_fraction\_leaf=0.0, n\_estimators=100,
                         presort='auto', random\_state=50, subsample=0.8, verbose=0,
                         warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=1,
                  param\_grid=[\{'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]\}],
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3604}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{)}
           \PY{k}{print} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'subsample': 0.8\}
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_38_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now cross validate with all the parameters set:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3614}]:} \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{7}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{9}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{11}\PY{p}{,} 
                           \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}
                          \PY{p}{\PYZcb{}}
           
           \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{]}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.05}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1500}\PY{p}{]}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.005}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
           \PY{p}{]}
           
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           
           \PY{n}{gbc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{gbc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3614}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=GradientBoostingClassifier(init=None, learning\_rate=0.1, loss='deviance',
                         max\_depth=9, max\_features=11, max\_leaf\_nodes=None,
                         min\_samples\_leaf=1, min\_samples\_split=7,
                         min\_weight\_fraction\_leaf=0.0, n\_estimators=100,
                         presort='auto', random\_state=50, subsample=0.8, verbose=0,
                         warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=1,
                  param\_grid=[\{'n\_estimators': [100], 'learning\_rate': [0.1]\}, \{'n\_estimators': [200], 'learning\_rate': [0.05]\}, \{'n\_estimators': [1000], 'learning\_rate': [0.01]\}, \{'n\_estimators': [1500], 'learning\_rate': [0.005]\}],
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3718}]:} \PY{k}{print} \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'n\_estimators': 1000, 'learning\_rate': 0.01\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3718}]:} [mean: 0.90460, std: 0.03726, params: \{'n\_estimators': 100, 'learning\_rate': 0.1\},
            mean: 0.90313, std: 0.03876, params: \{'n\_estimators': 200, 'learning\_rate': 0.05\},
            mean: 0.90830, std: 0.03889, params: \{'n\_estimators': 1000, 'learning\_rate': 0.01\},
            mean: 0.90370, std: 0.03988, params: \{'n\_estimators': 1500, 'learning\_rate': 0.005\}]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3854}]:} \PY{n}{p} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{gbc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Value}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}
           \PY{n}{p}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Parameter}\PY{l+s}{\PYZdq{}}
           \PY{k}{print} \PY{n}{p}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{ll\}
\textbackslash{}toprule
\{\} \&     Value \textbackslash{}\textbackslash{}
Parameter                \&           \textbackslash{}\textbackslash{}
\textbackslash{}midrule
init                     \&      None \textbackslash{}\textbackslash{}
learning\textbackslash{}\_rate            \&      0.01 \textbackslash{}\textbackslash{}
loss                     \&  deviance \textbackslash{}\textbackslash{}
max\textbackslash{}\_depth                \&         9 \textbackslash{}\textbackslash{}
max\textbackslash{}\_features             \&        11 \textbackslash{}\textbackslash{}
max\textbackslash{}\_leaf\textbackslash{}\_nodes           \&      None \textbackslash{}\textbackslash{}
min\textbackslash{}\_samples\textbackslash{}\_leaf         \&         1 \textbackslash{}\textbackslash{}
min\textbackslash{}\_samples\textbackslash{}\_split        \&         7 \textbackslash{}\textbackslash{}
min\textbackslash{}\_weight\textbackslash{}\_fraction\textbackslash{}\_leaf \&         0 \textbackslash{}\textbackslash{}
n\textbackslash{}\_estimators             \&      1000 \textbackslash{}\textbackslash{}
presort                  \&      auto \textbackslash{}\textbackslash{}
random\textbackslash{}\_state             \&        50 \textbackslash{}\textbackslash{}
subsample                \&       0.8 \textbackslash{}\textbackslash{}
verbose                  \&         0 \textbackslash{}\textbackslash{}
warm\textbackslash{}\_start               \&     False \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

    \subsubsection{AdaBoost Tuning}\label{adaboost-tuning}

Perhaps the easiest train due to a fairly limited number of parameters.
Adjusting the \texttt{max\_depth} suggests that 4 appears to be roughly
the best option for the depth of the decision trees.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3764}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.005}\PY{p}{]}\PY{p}{\PYZcb{}}
           
           \PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
           \PY{n}{adb} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{dt}\PY{p}{)}
           \PY{n}{adb\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{adb}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{adb\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3764}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=AdaBoostClassifier(algorithm='SAMME.R',
                     base\_estimator=DecisionTreeClassifier(class\_weight='balanced', criterion='gini', max\_depth=4,
                       max\_features=None, max\_leaf\_nodes=None, min\_samples\_leaf=1,
                       min\_samples\_split=2, min\_weight\_fraction\_leaf=0.0,
                       presort=False, random\_state=None, splitter='best'),
                     learning\_rate=0.1, n\_estimators=50, random\_state=None),
                  fit\_params=\{\}, iid=True, n\_jobs=-1,
                  param\_grid=\{'n\_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950], 'learning\_rate': [0.1, 0.5, 0.01, 0.005]\},
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3781}]:} \PY{k}{print} \PY{n}{adb\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{k}{print} \PY{n}{adb\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
           \PY{n}{adb\PYZus{}clf}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'n\_estimators': 400, 'learning\_rate': 0.5\}
0.898625942927
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3781}]:} [mean: 0.81604, std: 0.09086, params: \{'n\_estimators': 50, 'learning\_rate': 0.1\},
            mean: 0.82380, std: 0.06706, params: \{'n\_estimators': 100, 'learning\_rate': 0.1\},
            mean: 0.82011, std: 0.06168, params: \{'n\_estimators': 150, 'learning\_rate': 0.1\},
            mean: 0.83886, std: 0.05610, params: \{'n\_estimators': 200, 'learning\_rate': 0.1\},
            mean: 0.82707, std: 0.05400, params: \{'n\_estimators': 250, 'learning\_rate': 0.1\},
            mean: 0.83609, std: 0.05725, params: \{'n\_estimators': 300, 'learning\_rate': 0.1\},
            mean: 0.85398, std: 0.05896, params: \{'n\_estimators': 350, 'learning\_rate': 0.1\},
            mean: 0.83988, std: 0.06063, params: \{'n\_estimators': 400, 'learning\_rate': 0.1\},
            mean: 0.85468, std: 0.04492, params: \{'n\_estimators': 450, 'learning\_rate': 0.1\},
            mean: 0.84810, std: 0.03408, params: \{'n\_estimators': 500, 'learning\_rate': 0.1\},
            mean: 0.84320, std: 0.03759, params: \{'n\_estimators': 550, 'learning\_rate': 0.1\},
            mean: 0.86687, std: 0.06168, params: \{'n\_estimators': 600, 'learning\_rate': 0.1\},
            mean: 0.85343, std: 0.05496, params: \{'n\_estimators': 650, 'learning\_rate': 0.1\},
            mean: 0.84681, std: 0.04085, params: \{'n\_estimators': 700, 'learning\_rate': 0.1\},
            mean: 0.84218, std: 0.06393, params: \{'n\_estimators': 750, 'learning\_rate': 0.1\},
            mean: 0.83508, std: 0.05867, params: \{'n\_estimators': 800, 'learning\_rate': 0.1\},
            mean: 0.83509, std: 0.03651, params: \{'n\_estimators': 850, 'learning\_rate': 0.1\},
            mean: 0.84979, std: 0.05202, params: \{'n\_estimators': 900, 'learning\_rate': 0.1\},
            mean: 0.83524, std: 0.03882, params: \{'n\_estimators': 950, 'learning\_rate': 0.1\},
            mean: 0.85616, std: 0.05094, params: \{'n\_estimators': 50, 'learning\_rate': 0.5\},
            mean: 0.88002, std: 0.04807, params: \{'n\_estimators': 100, 'learning\_rate': 0.5\},
            mean: 0.86445, std: 0.05526, params: \{'n\_estimators': 150, 'learning\_rate': 0.5\},
            mean: 0.87179, std: 0.04243, params: \{'n\_estimators': 200, 'learning\_rate': 0.5\},
            mean: 0.87571, std: 0.04440, params: \{'n\_estimators': 250, 'learning\_rate': 0.5\},
            mean: 0.88136, std: 0.04933, params: \{'n\_estimators': 300, 'learning\_rate': 0.5\},
            mean: 0.86297, std: 0.04431, params: \{'n\_estimators': 350, 'learning\_rate': 0.5\},
            mean: 0.89863, std: 0.03614, params: \{'n\_estimators': 400, 'learning\_rate': 0.5\},
            mean: 0.87606, std: 0.03936, params: \{'n\_estimators': 450, 'learning\_rate': 0.5\},
            mean: 0.87972, std: 0.05611, params: \{'n\_estimators': 500, 'learning\_rate': 0.5\},
            mean: 0.88111, std: 0.02830, params: \{'n\_estimators': 550, 'learning\_rate': 0.5\},
            mean: 0.88124, std: 0.04582, params: \{'n\_estimators': 600, 'learning\_rate': 0.5\},
            mean: 0.87828, std: 0.04418, params: \{'n\_estimators': 650, 'learning\_rate': 0.5\},
            mean: 0.86320, std: 0.04748, params: \{'n\_estimators': 700, 'learning\_rate': 0.5\},
            mean: 0.88115, std: 0.03266, params: \{'n\_estimators': 750, 'learning\_rate': 0.5\},
            mean: 0.88156, std: 0.03102, params: \{'n\_estimators': 800, 'learning\_rate': 0.5\},
            mean: 0.87483, std: 0.03801, params: \{'n\_estimators': 850, 'learning\_rate': 0.5\},
            mean: 0.88320, std: 0.03020, params: \{'n\_estimators': 900, 'learning\_rate': 0.5\},
            mean: 0.88671, std: 0.03463, params: \{'n\_estimators': 950, 'learning\_rate': 0.5\},
            mean: 0.72016, std: 0.10635, params: \{'n\_estimators': 50, 'learning\_rate': 0.01\},
            mean: 0.76391, std: 0.09382, params: \{'n\_estimators': 100, 'learning\_rate': 0.01\},
            mean: 0.75591, std: 0.11900, params: \{'n\_estimators': 150, 'learning\_rate': 0.01\},
            mean: 0.75888, std: 0.12449, params: \{'n\_estimators': 200, 'learning\_rate': 0.01\},
            mean: 0.72914, std: 0.09521, params: \{'n\_estimators': 250, 'learning\_rate': 0.01\},
            mean: 0.75434, std: 0.07796, params: \{'n\_estimators': 300, 'learning\_rate': 0.01\},
            mean: 0.76913, std: 0.06759, params: \{'n\_estimators': 350, 'learning\_rate': 0.01\},
            mean: 0.76410, std: 0.08484, params: \{'n\_estimators': 400, 'learning\_rate': 0.01\},
            mean: 0.77088, std: 0.08914, params: \{'n\_estimators': 450, 'learning\_rate': 0.01\},
            mean: 0.75380, std: 0.09167, params: \{'n\_estimators': 500, 'learning\_rate': 0.01\},
            mean: 0.79806, std: 0.08337, params: \{'n\_estimators': 550, 'learning\_rate': 0.01\},
            mean: 0.77566, std: 0.09159, params: \{'n\_estimators': 600, 'learning\_rate': 0.01\},
            mean: 0.76275, std: 0.09476, params: \{'n\_estimators': 650, 'learning\_rate': 0.01\},
            mean: 0.79606, std: 0.09143, params: \{'n\_estimators': 700, 'learning\_rate': 0.01\},
            mean: 0.79669, std: 0.09392, params: \{'n\_estimators': 750, 'learning\_rate': 0.01\},
            mean: 0.78803, std: 0.08860, params: \{'n\_estimators': 800, 'learning\_rate': 0.01\},
            mean: 0.79535, std: 0.08959, params: \{'n\_estimators': 850, 'learning\_rate': 0.01\},
            mean: 0.80631, std: 0.09250, params: \{'n\_estimators': 900, 'learning\_rate': 0.01\},
            mean: 0.80001, std: 0.08379, params: \{'n\_estimators': 950, 'learning\_rate': 0.01\},
            mean: 0.76647, std: 0.09119, params: \{'n\_estimators': 50, 'learning\_rate': 0.005\},
            mean: 0.75694, std: 0.11073, params: \{'n\_estimators': 100, 'learning\_rate': 0.005\},
            mean: 0.75762, std: 0.10184, params: \{'n\_estimators': 150, 'learning\_rate': 0.005\},
            mean: 0.77939, std: 0.10277, params: \{'n\_estimators': 200, 'learning\_rate': 0.005\},
            mean: 0.78979, std: 0.09014, params: \{'n\_estimators': 250, 'learning\_rate': 0.005\},
            mean: 0.77683, std: 0.08775, params: \{'n\_estimators': 300, 'learning\_rate': 0.005\},
            mean: 0.75723, std: 0.07152, params: \{'n\_estimators': 350, 'learning\_rate': 0.005\},
            mean: 0.76043, std: 0.07355, params: \{'n\_estimators': 400, 'learning\_rate': 0.005\},
            mean: 0.75747, std: 0.07594, params: \{'n\_estimators': 450, 'learning\_rate': 0.005\},
            mean: 0.75407, std: 0.07616, params: \{'n\_estimators': 500, 'learning\_rate': 0.005\},
            mean: 0.78996, std: 0.07956, params: \{'n\_estimators': 550, 'learning\_rate': 0.005\},
            mean: 0.76600, std: 0.07753, params: \{'n\_estimators': 600, 'learning\_rate': 0.005\},
            mean: 0.79512, std: 0.08854, params: \{'n\_estimators': 650, 'learning\_rate': 0.005\},
            mean: 0.77854, std: 0.07721, params: \{'n\_estimators': 700, 'learning\_rate': 0.005\},
            mean: 0.77627, std: 0.07821, params: \{'n\_estimators': 750, 'learning\_rate': 0.005\},
            mean: 0.75797, std: 0.06653, params: \{'n\_estimators': 800, 'learning\_rate': 0.005\},
            mean: 0.79347, std: 0.08544, params: \{'n\_estimators': 850, 'learning\_rate': 0.005\},
            mean: 0.77119, std: 0.07737, params: \{'n\_estimators': 900, 'learning\_rate': 0.005\},
            mean: 0.76420, std: 0.05808, params: \{'n\_estimators': 950, 'learning\_rate': 0.005\}]
\end{Verbatim}
        
    \subsubsection{Extremely Random Trees
Tuning}\label{extremely-random-trees-tuning}

This is very similar to Random Forests. In fact we will start with the
same parameter set for the grid search.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3800}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                         \PY{l+s}{\PYZdq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                        \PY{p}{\PYZcb{}}
           \PY{n}{etc} \PY{o}{=} \PY{n}{ExtraTreesClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{bootstrap}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
           \PY{n}{etc\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{etc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{etc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3800}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=ExtraTreesClassifier(bootstrap=True, class\_weight='balanced',
                      criterion='gini', max\_depth=None, max\_features='auto',
                      max\_leaf\_nodes=None, min\_samples\_leaf=1, min\_samples\_split=2,
                      min\_weight\_fraction\_leaf=0.0, n\_estimators=50, n\_jobs=1,
                      oob\_score=False, random\_state=50, verbose=0, warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=-1,
                  param\_grid=\{'max\_features': [1, 4, 7, 10, 13, 16, 19], 'min\_samples\_split': [1, 2, 3, 4], 'max\_depth': [1, 4, 7, 10, 13, 16, 19], 'min\_samples\_leaf': [1, 2, 3, 4]\},
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3801}]:} \PY{k}{print} \PY{n}{etc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{k}{print} \PY{n}{etc\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'max\_features': 16, 'min\_samples\_split': 1, 'max\_depth': 19, 'min\_samples\_leaf': 1\}
0.887051406553
    \end{Verbatim}

    Now check increasing the number of estimators and find the drop off
point

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3805}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{\PYZcb{}}
           \PY{n}{const\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{16}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{19}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}
           
           \PY{n}{etc} \PY{o}{=} \PY{n}{ExtraTreesClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{bootstrap}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{const\PYZus{}params}\PY{p}{)}
           \PY{n}{etc\PYZus{}clf2} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{etc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{skf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{roc\PYZus{}auc}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3805}]:} GridSearchCV(cv=sklearn.cross\_validation.StratifiedKFold(labels=[1 0 {\ldots}, 0 0], n\_folds=5, shuffle=False, random\_state=None),
                  error\_score='raise',
                  estimator=ExtraTreesClassifier(bootstrap=True, class\_weight='balanced',
                      criterion='gini', max\_depth=19, max\_features=16,
                      max\_leaf\_nodes=None, min\_samples\_leaf=1, min\_samples\_split=1,
                      min\_weight\_fraction\_leaf=0.0, n\_estimators=10, n\_jobs=1,
                      oob\_score=False, random\_state=50, verbose=0, warm\_start=False),
                  fit\_params=\{\}, iid=True, n\_jobs=-1,
                  param\_grid=\{'n\_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450]\},
                  pre\_dispatch='2*n\_jobs', refit=True, scoring='roc\_auc', verbose=0)
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3806}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{d}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{]}\PY{p}{)}
           \PY{k}{print} \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
           \PY{k}{print} \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'n\_estimators': 200\}
0.890377576778
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3807}]:} \PY{n}{etc\PYZus{}clf2}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3807}]:} \{'bootstrap': True,
            'class\_weight': 'balanced',
            'criterion': 'gini',
            'max\_depth': 19,
            'max\_features': 16,
            'max\_leaf\_nodes': None,
            'min\_samples\_leaf': 1,
            'min\_samples\_split': 1,
            'min\_weight\_fraction\_leaf': 0.0,
            'n\_estimators': 200,
            'n\_jobs': 1,
            'oob\_score': False,
            'random\_state': 50,
            'verbose': 0,
            'warm\_start': False\}
\end{Verbatim}
        
    \subsection{Model Performance}\label{model-performance}

Test the performance of each of the models on the preprocessed dataset
before trying any more complicated feature engineering/resampling. This
should give us some rough baseline AUC measures to work with. Firstly,
set up the models. This creates a set of pipelines for each of the
models we want to use.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3971}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.pipeline} \PY{k+kn}{import} \PY{n}{Pipeline}
           \PY{k+kn}{from} \PY{n+nn}{sklearn.svm} \PY{k+kn}{import} \PY{n}{SVC}
           \PY{k+kn}{from} \PY{n+nn}{sklearn.ensemble} \PY{k+kn}{import} \PY{n}{ExtraTreesClassifier}\PY{p}{,} \PY{n}{AdaBoostClassifier}
           \PY{k+kn}{from} \PY{n+nn}{sklearn.neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier}
           \PY{k+kn}{from} \PY{n+nn}{sklearn.tree} \PY{k+kn}{import} \PY{n}{DecisionTreeClassifier}
           \PY{n+nb}{reload}\PY{p}{(}\PY{n}{pipeline}\PY{p}{)}
           \PY{k+kn}{import} \PY{n+nn}{pipeline}
           \PY{n+nb}{reload}\PY{p}{(}\PY{n}{roc\PYZus{}analysis}\PY{p}{)}
           \PY{k+kn}{from} \PY{n+nn}{roc\PYZus{}analysis} \PY{k+kn}{import} \PY{n}{ROCAnalysisScorer}
           
           \PY{n}{scaler} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
           
           \PY{c}{\PYZsh{} set up classifier objects}
           \PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{weights}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{distance}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{dct} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
           \PY{n}{abt} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{dct}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
           
           \PY{n}{gbc\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}} 
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} 
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{7}\PY{p}{,} 
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{9}\PY{p}{,} 
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{11}\PY{p}{,} 
               \PY{l+s}{\PYZsq{}}\PY{l+s}{subsample}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1000}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{learning\PYZus{}rate}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.01}
           \PY{p}{\PYZcb{}}
           \PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{gbc\PYZus{}params}\PY{p}{)}
           
           \PY{n}{exf\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{bootstrap}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{True}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{class\PYZus{}weight}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{criterion}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{gini}\PY{l+s}{\PYZsq{}}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{19}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{16}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}leaf\PYZus{}nodes}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{None}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}weight\PYZus{}fraction\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.0}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{200}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}jobs}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{oob\PYZus{}score}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{False}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{random\PYZus{}state}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{50}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{verbose}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{warm\PYZus{}start}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{False}
           \PY{p}{\PYZcb{}}
           
           
           \PY{n}{exf} \PY{o}{=} \PY{n}{ExtraTreesClassifier}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{exf\PYZus{}params}\PY{p}{)}
           
           \PY{n}{rf\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{bootstrap}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{True}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{class\PYZus{}weight}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{balanced}\PY{l+s}{\PYZsq{}}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{criterion}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{gini}\PY{l+s}{\PYZsq{}}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}depth}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{16}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}features}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{max\PYZus{}leaf\PYZus{}nodes}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{None}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}samples\PYZus{}split}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{min\PYZus{}weight\PYZus{}fraction\PYZus{}leaf}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.0}\PY{p}{,}
               \PY{l+s}{\PYZsq{}}\PY{l+s}{n\PYZus{}estimators}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{300}
           \PY{p}{\PYZcb{}}
           \PY{n}{rf\PYZus{}balanced} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{rf\PYZus{}params}\PY{p}{)}
           
           \PY{c}{\PYZsh{} create pipelines for each model}
           \PY{n}{abt\PYZus{}pipe} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{scaler}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{scaler}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{abt}\PY{p}{)}\PY{p}{]}\PY{p}{)}
           \PY{n}{exf\PYZus{}pipe} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{scaler}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{scaler}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{ExtraTrees}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{exf}\PY{p}{)}\PY{p}{]}\PY{p}{)}
           \PY{n}{gbc\PYZus{}pipe} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{scaler}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{scaler}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{GradientBoostingClassifer}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{gbc}\PY{p}{)}\PY{p}{]}\PY{p}{)}
           \PY{n}{rfs\PYZus{}pipe} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{scaler}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{scaler}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{RandomForest}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{rf\PYZus{}balanced}\PY{p}{)}\PY{p}{]}\PY{p}{)}
           
           \PY{c}{\PYZsh{} create list of model data}
           \PY{n}{models} \PY{o}{=} \PY{p}{[}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{name}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{model}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n}{abt\PYZus{}pipe}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{name}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{ExtraTrees}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{model}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n}{exf\PYZus{}pipe}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{name}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{RandomForest}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{model}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n}{rfs\PYZus{}pipe}\PY{p}{\PYZcb{}}\PY{p}{,}
               \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{name}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{GradientBoost}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{model}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{n}{gbc\PYZus{}pipe}\PY{p}{\PYZcb{}}\PY{p}{,}
           \PY{p}{]}
           
           \PY{c}{\PYZsh{} set the same training set for all models.}
           \PY{c}{\PYZsh{} this is just the preprocessed dataset.}
           \PY{k}{for} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
               \PY{n}{model}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{train\PYZus{}data}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
\end{Verbatim}

    Define some useful helper functions for summarising the results of
k-fold/monte carlo cross validation

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3829}]:} \PY{k}{def} \PY{n+nf}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}\PY{p}{:}
               \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Create a summary of the average f\PYZhy{}scores for all folds/trials\PYZdq{}\PYZdq{}\PYZdq{}}
               \PY{n}{series} \PY{o}{=} \PY{p}{[}\PY{p}{]}
               \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{p}{]}
               \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                   \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scorer}\PY{o}{.}\PY{n}{f1scores\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scorer}\PY{o}{.}\PY{n}{f2scores\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scorer}\PY{o}{.}\PY{n}{fhalf\PYZus{}scores\PYZus{}}\PY{p}{)}\PY{p}{]}
                   \PY{n}{s} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{f\PYZus{}scores}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{F1}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{F2}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{F0.5}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                   \PY{n}{series}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{s}\PY{p}{)}
                   \PY{n}{columns}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{key}\PY{p}{)}
               
               \PY{n}{frame} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{series}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}    
               \PY{n}{frame}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n}{columns}
               \PY{k}{return} \PY{n}{frame}
           
           \PY{k}{def} \PY{n+nf}{summarise\PYZus{}scorers}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}\PY{p}{:}
               \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Create a summary of the scorers AUCs for all folds/trials\PYZdq{}\PYZdq{}\PYZdq{}}
               \PY{n}{names} \PY{o}{=} \PY{p}{[}\PY{n}{name} \PY{k}{for} \PY{n}{name} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{]}
               \PY{n}{aucs} \PY{o}{=} \PY{p}{[}\PY{n}{scorer}\PY{o}{.}\PY{n}{aucs\PYZus{}} \PY{k}{for} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{values}\PY{p}{(}\PY{p}{)}\PY{p}{]}
               \PY{n}{aucs} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{aucs}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{names}\PY{p}{)}
               \PY{k}{return} \PY{n}{aucs}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    Perform n iterations of k fold cross validation. Here I am using 10
iterations and 5 folds at each iteration.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3972}]:} \PY{n}{scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{repeated\PYZus{}cross\PYZus{}fold\PYZus{}validation}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    Plot an ROC curve and the mean AUCs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3973}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
           \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{c}{\PYZsh{} plt.savefig(\PYZdq{}img/roc\PYZus{}cv.png\PYZdq{})}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3973}]:} [<matplotlib.lines.Line2D at 0x16be3da10>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Plot bar chart of the F2 scores

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3862}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{n}{ax} \PY{o}{=} \PY{n}{f\PYZus{}scores}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{F2}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{barh}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{F2 Measure for All Classifiers}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{b}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{r}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{g}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{y}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
           \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{F2 Score}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{img/f2\PYZus{}score.png}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Summarise the F scores

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3833}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{k}{print} \PY{n}{f\PYZus{}scores}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
           \PY{n}{f\PYZus{}scores}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{lrrrr\}
\textbackslash{}toprule
\{\} \&  RandomForest \&  ExtraTrees \&  GradientBoost \&  AdaBoost \textbackslash{}\textbackslash{}
\textbackslash{}midrule
F1   \&      0.601006 \&    0.606542 \&       0.623161 \&  0.607031 \textbackslash{}\textbackslash{}
F2   \&      0.522676 \&    0.546392 \&       0.567522 \&  0.549125 \textbackslash{}\textbackslash{}
F0.5 \&      0.715848 \&    0.688120 \&       0.700784 \&  0.683489 \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3833}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.601006    0.606542       0.623161  0.607031
           F2        0.522676    0.546392       0.567522  0.549125
           F0.5      0.715848    0.688120       0.700784  0.683489
\end{Verbatim}
        
    \subsection{Feature Engineering}\label{feature-engineering}

Test creating some new features based on combinations of existing ones
in the dataset. Cross validate each set of new features to see if it
improves performance.

    \subsubsection{Binary Features}\label{binary-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3917}]:} \PY{k+kn}{import} \PY{n+nn}{itertools}
           
           \PY{k}{def} \PY{n+nf}{binary\PYZus{}combinations}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{names}\PY{p}{)}\PY{p}{:}
               \PY{n}{name\PYZus{}pairs} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{combinations}\PY{p}{(}\PY{n}{names}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
               \PY{n}{features} \PY{o}{=} \PY{p}{[}\PY{p}{]}
               \PY{k}{for} \PY{n}{a\PYZus{}name}\PY{p}{,} \PY{n}{b\PYZus{}name} \PY{o+ow}{in} \PY{n}{name\PYZus{}pairs}\PY{p}{:}
                   \PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{a\PYZus{}name}\PY{p}{]}\PY{p}{,} \PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{b\PYZus{}name}\PY{p}{]}
                   \PY{n}{features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}xor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}\PY{p}{)}
                   \PY{n}{features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}and}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}\PY{p}{)}
                   \PY{n}{features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}or}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}\PY{p}{)}
                   
               \PY{k}{return} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{)}
           
           \PY{n}{binary\PYZus{}features} \PY{o}{=} \PY{n}{binary\PYZus{}combinations}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{PRE7}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE8}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE9}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE10}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE11}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE17}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE30}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
           \PY{n}{Xp\PYZus{}binary} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xp}\PY{p}{,} \PY{n}{binary\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{measure\PYZus{}importance}\PY{p}{(}\PY{n}{Xp\PYZus{}binary}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           \PY{n}{Xp\PYZus{}binary}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{[}\PY{n}{feature\PYZus{}importance} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3879}]:} \PY{k}{for} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
               \PY{n}{model}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{train\PYZus{}data}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{Xp\PYZus{}binary}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
               
           \PY{n}{scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{repeated\PYZus{}cross\PYZus{}fold\PYZus{}validation}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3881}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
           \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/roc\PYZus{}binary\PYZus{}features.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_69_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3882}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{k}{print} \PY{n}{f\PYZus{}scores}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
           \PY{n}{f\PYZus{}scores}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{lrrrr\}
\textbackslash{}toprule
\{\} \&  RandomForest \&  ExtraTrees \&  GradientBoost \&  AdaBoost \textbackslash{}\textbackslash{}
\textbackslash{}midrule
F1   \&      0.543811 \&    0.560718 \&       0.612539 \&  0.646753 \textbackslash{}\textbackslash{}
F2   \&      0.465938 \&    0.489906 \&       0.547107 \&  0.591129 \textbackslash{}\textbackslash{}
F0.5 \&      0.662682 \&    0.660713 \&       0.705097 \&  0.721499 \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3882}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.543811    0.560718       0.612539  0.646753
           F2        0.465938    0.489906       0.547107  0.591129
           F0.5      0.662682    0.660713       0.705097  0.721499
\end{Verbatim}
        
    \subsubsection{Spirometry Based
Features}\label{spirometry-based-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4031}]:} \PY{k}{def} \PY{n+nf}{create\PYZus{}spiro\PYZus{}features}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{)}\PY{p}{:}
               \PY{c}{\PYZsh{} create new feature FER}
               \PY{c}{\PYZsh{} this is the raito of FEV1 and FVC}
               \PY{n}{FER} \PY{o}{=} \PY{p}{(}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{PRE5} \PY{o}{/} \PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{PRE4}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}
               \PY{n}{FER}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{index}
           
               \PY{c}{\PYZsh{} create a new feature OBS}
               \PY{c}{\PYZsh{} this is whether the instance has a FER below 70\PYZpc{}}
               \PY{c}{\PYZsh{} which implies an obstructive disease.}
               \PY{n}{OBS} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{AGE}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
               \PY{n}{OBS}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{index}
               \PY{n}{OBS}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{FER} \PY{o}{\PYZlt{}} \PY{l+m+mi}{70}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{1.0}
               
               \PY{n}{spiro} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{FER}\PY{p}{,} \PY{n}{OBS}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
               \PY{n}{spiro}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{FER}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{OBS}\PY{l+s}{\PYZsq{}}\PY{p}{]}
               \PY{k}{return} \PY{n}{spiro}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4032}]:} \PY{n}{spiro\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}spiro\PYZus{}features}\PY{p}{(}\PY{n}{Xp}\PY{p}{)}
           \PY{n}{Xp\PYZus{}spiro} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xp}\PY{p}{,} \PY{n}{spiro\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{measure\PYZus{}importance}\PY{p}{(}\PY{n}{Xp\PYZus{}spiro}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           \PY{n}{Xp\PYZus{}spiro}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{[}\PY{n}{feature\PYZus{}importance} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_73_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3954}]:} \PY{k}{for} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
               \PY{n}{model}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{train\PYZus{}data}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{Xp\PYZus{}spiro}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
               
           \PY{n}{scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{repeated\PYZus{}cross\PYZus{}fold\PYZus{}validation}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3955}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
           \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/roc\PYZus{}spiro\PYZus{}features.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_75_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3957}]:} \PY{n}{feature\PYZus{}importance}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{barh}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Gini Impurity}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/importance\PYZus{}spiro\PYZus{}features.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3958}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{k}{print} \PY{n}{f\PYZus{}scores}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
           \PY{n}{f\PYZus{}scores}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{lrrrr\}
\textbackslash{}toprule
\{\} \&  RandomForest \&  ExtraTrees \&  GradientBoost \&  AdaBoost \textbackslash{}\textbackslash{}
\textbackslash{}midrule
F1   \&      0.566669 \&    0.617225 \&       0.600845 \&  0.614050 \textbackslash{}\textbackslash{}
F2   \&      0.483774 \&    0.549281 \&       0.538619 \&  0.561441 \textbackslash{}\textbackslash{}
F0.5 \&      0.699297 \&    0.715047 \&       0.688793 \&  0.683218 \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3958}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.566669    0.617225       0.600845  0.614050
           F2        0.483774    0.549281       0.538619  0.561441
           F0.5      0.699297    0.715047       0.688793  0.683218
\end{Verbatim}
        
    \subsubsection{Polynomial Combinations}\label{polynomial-combinations}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3959}]:} \PY{k}{def} \PY{n+nf}{create\PYZus{}poly\PYZus{}features}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{,} \PY{n}{names}\PY{p}{)}\PY{p}{:}
               \PY{c}{\PYZsh{} create new features base on Polynomials of the original best two predictors}
               \PY{n}{poly} \PY{o}{=} \PY{n}{sklearn}\PY{o}{.}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{PolynomialFeatures}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{include\PYZus{}bias}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{,} \PY{n}{interaction\PYZus{}only}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
               \PY{n}{poly\PYZus{}features} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{poly}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{x\PYZus{}data}\PY{p}{[}\PY{n}{names}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{x\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{)}
               \PY{n}{poly\PYZus{}features}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{POLY\PYZus{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{poly\PYZus{}features}\PY{o}{.}\PY{n}{columns}\PY{p}{]}
               \PY{k}{return} \PY{n}{poly\PYZus{}features}
           
           \PY{n}{poly\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}poly\PYZus{}features}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{PRE4}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE5}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
           \PY{n}{Xp\PYZus{}poly} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xp}\PY{p}{,} \PY{n}{poly\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{measure\PYZus{}importance}\PY{p}{(}\PY{n}{Xp\PYZus{}poly}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           \PY{n}{Xp\PYZus{}poly}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{[}\PY{n}{feature\PYZus{}importance} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_79_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3960}]:} \PY{k}{for} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
               \PY{n}{model}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{train\PYZus{}data}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{Xp\PYZus{}poly}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
               
           \PY{n}{scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{repeated\PYZus{}cross\PYZus{}fold\PYZus{}validation}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3961}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
           \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
           
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/roc\PYZus{}poly\PYZus{}features.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_81_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3962}]:} \PY{n}{feature\PYZus{}importance}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{barh}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Gini Impurity}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/importance\PYZus{}poly\PYZus{}features.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_82_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3964}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{k}{print} \PY{n}{f\PYZus{}scores}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
           \PY{n}{f\PYZus{}scores}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{lrrrr\}
\textbackslash{}toprule
\{\} \&  RandomForest \&  ExtraTrees \&  GradientBoost \&  AdaBoost \textbackslash{}\textbackslash{}
\textbackslash{}midrule
F1   \&      0.604371 \&    0.634325 \&       0.615442 \&  0.662434 \textbackslash{}\textbackslash{}
F2   \&      0.517592 \&    0.563459 \&       0.560582 \&  0.613066 \textbackslash{}\textbackslash{}
F0.5 \&      0.736327 \&    0.732681 \&       0.690843 \&  0.726515 \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3964}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.604371    0.634325       0.615442  0.662434
           F2        0.517592    0.563459       0.560582  0.613066
           F0.5      0.736327    0.732681       0.690843  0.726515
\end{Verbatim}
        
    \subsection{Resampling the Dataset}\label{resampling-the-dataset}

Testing whether using resampling improves performance

    \subsubsection{Testing with regular Over/Under
sampling}\label{testing-with-regular-overunder-sampling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3906}]:} \PY{n}{splitter} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{OverUnderSplitter}\PY{p}{(}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{under\PYZus{}sample}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{over\PYZus{}sample}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{)}
           \PY{n}{overunder\PYZus{}scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{monte\PYZus{}carlo\PYZus{}validation}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{,} \PY{n}{models}\PY{p}{,} \PY{n}{splitter}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Over-sampling performed: Counter(\{0: 167, 1: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 120\}
Under-sampling performed: Counter(\{1: 120, 0: 48\})
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3822}]:} \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{overunder\PYZus{}scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_87_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3823}]:} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{overunder\PYZus{}scorers}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3823}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.573963    0.585601       0.554825  0.540944
           F2        0.726930    0.719755       0.672136  0.645951
           F0.5      0.474689    0.494410       0.473304  0.466582
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3824}]:} \PY{n}{summarise\PYZus{}scorers}\PY{p}{(}\PY{n}{smote\PYZus{}scorers}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3824}]:}        RandomForest  ExtraTrees  GradientBoost   AdaBoost
           count     50.000000   50.000000      50.000000  50.000000
           mean       0.828473    0.838683       0.833866   0.821345
           std        0.053028    0.050751       0.054822   0.059401
           min        0.661064    0.672269       0.665266   0.691877
           25\%        0.802871    0.789041       0.798319   0.790966
           50\%        0.823179    0.852591       0.832633   0.822829
           75\%        0.857843    0.874125       0.869398   0.860994
           max        0.929272    0.936975       0.946779   0.959384
\end{Verbatim}
        
    \subsubsection{Testing with SMOTE +
Undersampling}\label{testing-with-smote-undersampling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3907}]:} \PY{n}{smote\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{kind}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{regular}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{k}\PY{l+s}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{ratio}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{verbose}\PY{l+s}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}
           \PY{n}{splitter} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{SMOTESplitter}\PY{p}{(}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{under\PYZus{}sample}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{smote\PYZus{}params}\PY{o}{=}\PY{n}{smote\PYZus{}params}\PY{p}{)}
           \PY{n}{smote\PYZus{}scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{monte\PYZus{}carlo\PYZus{}validation}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{n}{Yp}\PY{p}{,} \PY{n}{models}\PY{p}{,} \PY{n}{splitter}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
Determining classes statistics{\ldots} 2 classes detected: \{0: 167, 1: 67\}
Finding the 3 nearest neighbours{\ldots}done!
Creating synthetic samples{\ldots}Generated 53 new samples {\ldots}
done!
Determining classes statistics{\ldots} 2 classes detected: \{0.0: 167, 1.0: 120\}
Under-sampling performed: Counter(\{0.0: 120, 1.0: 120\})
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3910}]:} \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{smote\PYZus{}scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
               
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
           \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{img/roc\PYZus{}smote.png}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_92_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3911}]:} \PY{n}{smote\PYZus{}f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{smote\PYZus{}scorers}\PY{p}{)}
           \PY{k}{print} \PY{n}{smote\PYZus{}f\PYZus{}scores}\PY{o}{.}\PY{n}{to\PYZus{}latex}\PY{p}{(}\PY{p}{)}
           \PY{n}{smote\PYZus{}f\PYZus{}scores}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbackslash{}begin\{tabular\}\{lrrrr\}
\textbackslash{}toprule
\{\} \&  RandomForest \&  ExtraTrees \&  GradientBoost \&  AdaBoost \textbackslash{}\textbackslash{}
\textbackslash{}midrule
F1   \&      0.614570 \&    0.613840 \&       0.615768 \&  0.611035 \textbackslash{}\textbackslash{}
F2   \&      0.648492 \&    0.650097 \&       0.648949 \&  0.643250 \textbackslash{}\textbackslash{}
F0.5 \&      0.587619 \&    0.585145 \&       0.589706 \&  0.586983 \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3911}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.614570    0.613840       0.615768  0.611035
           F2        0.648492    0.650097       0.648949  0.643250
           F0.5      0.587619    0.585145       0.589706  0.586983
\end{Verbatim}
        
    \subsection{Best Classifier}\label{best-classifier}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4060}]:} \PY{n}{spiro\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}spiro\PYZus{}features}\PY{p}{(}\PY{n}{Xp}\PY{p}{)}
           \PY{n}{poly\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}poly\PYZus{}features}\PY{p}{(}\PY{n}{Xp}\PY{p}{,} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{PRE4}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE5}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
           \PY{n}{Xp\PYZus{}all} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xp}\PY{p}{,} \PY{n}{poly\PYZus{}features}\PY{p}{,} \PY{n}{spiro\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           \PY{n}{Xp\PYZus{}all}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{DGN\PYZus{}1}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{DGN\PYZus{}8}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
           \PY{k}{for} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
               \PY{n}{model}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{train\PYZus{}data}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{Xp\PYZus{}all}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           
           \PY{n}{scorers} \PY{o}{=} \PY{n}{pipeline}\PY{o}{.}\PY{n}{repeated\PYZus{}cross\PYZus{}fold\PYZus{}validation}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4061}]:} \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{scorer} \PY{o+ow}{in} \PY{n}{scorers}\PY{o}{.}\PY{n}{iteritems}\PY{p}{(}\PY{p}{)}\PY{p}{:}
               \PY{n}{scorer}\PY{o}{.}\PY{n}{plot\PYZus{}roc\PYZus{}curve}\PY{p}{(}\PY{n}{mean\PYZus{}label}\PY{o}{=}\PY{n}{key}\PY{p}{,} \PY{n}{mean\PYZus{}line}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{show\PYZus{}all}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
               
           \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZhy{}\PYZhy{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4061}]:} [<matplotlib.lines.Line2D at 0x19f05aa90>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Analysis_files/Analysis_96_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3981}]:} \PY{n}{f\PYZus{}scores} \PY{o}{=} \PY{n}{f\PYZus{}score\PYZus{}summary}\PY{p}{(}\PY{n}{scorers}\PY{p}{)}
           \PY{n}{f\PYZus{}scores}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3981}]:}       RandomForest  ExtraTrees  GradientBoost  AdaBoost
           F1        0.559570    0.608095       0.617726  0.624809
           F2        0.472801    0.538665       0.556561  0.575048
           F0.5      0.699614    0.703232       0.699795  0.689190
\end{Verbatim}
        
    \subsection{Predicton on Test Set}\label{predicton-on-test-set}

Finally, based on the best combination of techniques used in the
preceeding sections, and using the classifier with the best AUC
performance, make probalistic predictions based on the unlabelled test
data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4083}]:} \PY{n}{Xtest}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{preprocess}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}
           \PY{n}{Xtest} \PY{o}{=} \PY{n}{Xtest}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{test\PYZus{}id}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           
           \PY{n}{test\PYZus{}spiro\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}spiro\PYZus{}features}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
           \PY{n}{test\PYZus{}poly\PYZus{}features} \PY{o}{=} \PY{n}{create\PYZus{}poly\PYZus{}features}\PY{p}{(}\PY{n}{Xtest}\PY{p}{,} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{PRE4}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{PRE5}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
           \PY{n}{Xtest} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{Xtest}\PY{p}{,} \PY{n}{test\PYZus{}spiro\PYZus{}features}\PY{p}{,} \PY{n}{test\PYZus{}poly\PYZus{}features}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           
           \PY{k}{print} \PY{n}{Xtest}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{size}\PY{p}{,} \PY{n}{Xp\PYZus{}all}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{size}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
24 24
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4084}]:} \PY{n}{gbc\PYZus{}final} \PY{o}{=} \PY{n}{models}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{model}\PY{l+s}{\PYZsq{}}\PY{p}{]}
           \PY{n}{gbc\PYZus{}final}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}all}\PY{p}{,} \PY{n}{Yp}\PY{p}{)}
           \PY{n}{predicted\PYZus{}prob} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{gbc\PYZus{}final}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4085}]:} \PY{n}{predicted\PYZus{}label} \PY{o}{=} \PY{n}{predicted\PYZus{}prob}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
           \PY{n}{predicted\PYZus{}label}\PY{p}{[}\PY{n}{predicted\PYZus{}label}\PY{o}{\PYZgt{}}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
           \PY{n}{predicted\PYZus{}label}\PY{p}{[}\PY{n}{predicted\PYZus{}label}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.5}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
           
           \PY{n}{final\PYZus{}submission} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{test}\PY{o}{.}\PY{n}{test\PYZus{}id}\PY{p}{,} \PY{n}{predicted\PYZus{}label}\PY{p}{,} \PY{n}{predicted\PYZus{}prob}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
           \PY{n}{final\PYZus{}submission}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{test\PYZus{}id}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{predicted\PYZus{}label}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{predicted\PYZus{}output}\PY{l+s}{\PYZsq{}}\PY{p}{]}
           \PY{n}{final\PYZus{}submission}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4085}]:}     test\_id  predicted\_label  predicted\_output
           0         1              1.0          0.948372
           1         2              1.0          0.672873
           2         3              0.0          0.413674
           3         4              0.0          0.326106
           4         5              0.0          0.056265
           5         6              1.0          0.834277
           6         7              1.0          0.806592
           7         8              1.0          0.729185
           8         9              1.0          0.864669
           9        10              0.0          0.017802
           10       11              1.0          0.848263
           11       12              1.0          0.810005
           12       13              0.0          0.115374
           13       14              1.0          0.538317
           14       15              1.0          0.649004
           15       16              1.0          0.779527
           16       17              0.0          0.456595
           17       18              1.0          0.878872
           18       19              1.0          0.881226
           19       20              0.0          0.450626
           20       21              1.0          0.584035
           21       22              0.0          0.224294
           22       23              0.0          0.289585
           23       24              1.0          0.698764
           24       25              1.0          0.966193
           25       26              1.0          0.914535
           26       27              0.0          0.321268
           27       28              1.0          0.860401
           28       29              1.0          0.740382
           29       30              1.0          0.695548
           ..      {\ldots}              {\ldots}               {\ldots}
           70       71              1.0          0.939841
           71       72              1.0          0.960235
           72       73              1.0          0.809710
           73       74              1.0          0.916319
           74       75              1.0          0.708242
           75       76              1.0          0.890017
           76       77              0.0          0.415826
           77       78              1.0          0.625773
           78       79              1.0          0.915442
           79       80              1.0          0.772913
           80       81              1.0          0.904518
           81       82              1.0          0.860633
           82       83              1.0          0.932651
           83       84              0.0          0.301895
           84       85              0.0          0.132390
           85       86              1.0          0.550989
           86       87              1.0          0.886839
           87       88              1.0          0.877086
           88       89              0.0          0.233983
           89       90              1.0          0.824413
           90       91              1.0          0.659459
           91       92              1.0          0.938119
           92       93              1.0          0.934539
           93       94              1.0          0.758052
           94       95              1.0          0.936733
           95       96              0.0          0.061116
           96       97              1.0          0.844611
           97       98              1.0          0.913141
           98       99              1.0          0.927978
           99      100              0.0          0.305664
           
           [100 rows x 3 columns]
\end{Verbatim}
        

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
